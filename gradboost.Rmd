---
title: "gradboost"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(furrr)
library(fastDummies)
library(MLmetrics)
library(modelr)
library(xgboost)
```
Custom function
```{r}
#

```



```{r}
result_sample <- read_csv("raw_data/sample_submission.csv", col_types = c("fd"))
.data_all <- read.csv("data/data_all.csv", 
                      na.strings = "",
                      stringsAsFactors = TRUE)
# fill-up missing data
data_all <- .data_all %>% 
  fill(gender:t_revenue, .direction = "updown") %>% 
  select(-index, -id)

# one hot encoding 
factor_cols <- names(data_all)[which(map_chr(data_all, class) == "factor")]
numeric_cols <- names(data_all)[which(map_chr(data_all, class) %in% c("double", "integer", "numeric"))]
data_all2 <- data_all %>% 
  dummy_cols(select_columns = head(factor_cols, -1), # not includ churn
             #remove_first_dummy = TRUE,
             remove_selected_columns = TRUE) %>% 
  mutate(across(.cols = !!numeric_cols, ~ scale(.x))) %>% 
  mutate(churn = factor(churn,
                        levels = c("No Churn", "Competitor", "Dissatisfaction", 
                                   "Attitude", "Price", "Other"),
                        labels = c(0, 1, 2, 3, 4, 5)) %>% as.character %>% as.integer())

# split data
data_test <- data_all2 %>% 
  filter(is_test == TRUE) %>% 
  select(-is_test, -is_train)
dim(data_test)

X_test <- data_test %>% select(-churn) %>% as.matrix()
y_test <- data_test$churn 

data_train <- data_all2 %>% 
  filter(is_train == TRUE, !is.na(churn)) %>% 
  select(-is_test, -is_train) 
dim(data_train)

X_train <- data_train %>% select(-churn) %>% as.matrix()
y_train <- data_train$churn 
```


```{r}
xgb_train <- xgb.DMatrix(data = X_train, label = y_train)
#xgb_test <- xgb.DMatrix(data = X_test, label = y_test)
xgb_params <- list(booster = "gbtree", 
  eta = 0.01, max_depth = 8, gamma = 4, subsample = 0.75, colsample_bytree = 1,
  objective = "multi:softmax", eval_metric = "mlogloss", num_class = 6
)

xgb_model <- xgb.cv(params = xgb_params, data = xgb_train,
                       nrounds = 1000, nfold = 3, verbose = 1)
```


```{r}
xgb_cv_train <- function(train, .eta, .gamma, .max_depth, .subsample) {
  .train <- train$data[train$idx, ]
  X_train <- .train %>% select(-churn) %>% as.matrix()
  y_train <- .train$churn
  
  xgb_train <- xgb.DMatrix(data = X_train, label = y_train)

  xgb_params <- list(booster = "gbtree", 
    eta = .eta, max_depth = .max_depth, gamma = .gamma, subsample = .subsample, 
    colsample_bytree = 1, objective = "multi:softmax", eval_metric = "mlogloss", num_class = 6
  )

  xgb_model <- xgb.train(params = xgb_params, data = xgb_train,
                         nrounds = 5000, verbose = 0)
}

xgb_cv_result <- function(model, val) {
  .val <- val$data[val$idx, ]
  X_val <- .val %>% select(-churn) %>% as.matrix()
  y_val <- .val$churn 
  y_hat <- predict(model, X_val)
  f1_score <- F1_Score(y_val, y_hat) 
}

xgb_cv <- function(data_train_cv, .eta, .gamma, .max_depth, .subsample) {
  model <- map(data_train_cv$train, ~ xgb_cv_train(., .eta, .gamma, .max_depth, .subsample))
  f1_scores <- map2_dbl(model, data_train_cv$test, ~ xgb_cv_result(.x, .y))
  mean_f1_score <- mean(f1_scores)
}

xgb_cv <- function(X_train, y_train, k, .eta, .gamma, .max_depth, .subsample) {
  
  xgb_train <- xgb.DMatrix(data = X_train, label = y_train)

  xgb_params <- list(booster = "gbtree", 
    eta = .eta, gamma = .gamma, max_depth = .max_depth, subsample = .subsample, 
    colsample_bytree = 1, objective = "multi:softmax", eval_metric = "mlogloss", num_class = 6
  )
  
  xgb_model <- xgb.cv(params = xgb_params, data = xgb_train,
                      nrounds = 1000, nfold = k, verbose = 0)
  
  return(xgb_model$evaluation_log[1000, ])
}
```

```{r}

eta <- 10^seq(-2, 0, by = 1/3) %>% head(-1)
gamma <- 10^seq(0, 1, by = 1/3) 
max_depth <- c(4, 8)
subsample <- c(0.75, 1)
par <- expand.grid(eta, gamma, max_depth, subsample)

result <- pmap_dfr(par, ~ xgb_cv(X_train, y_train, k = 5,
                                        .eta = ..1, .gamma = ..2, 
                                        .max_depth = ..3, .subsample = ..4))
# result2 <- pmap_dfr(par[33:64,], ~ xgb_cv(X_train, y_train, k = 5,
#                                         .eta = ..1, .gamma = ..2, 
#                                         .max_depth = ..3, .subsample = ..4))

# result <- rbind(result1, result2)
# View(result)
write_csv(cbind(par, result), "prediction/xgb_cv_result2.csv")

```

```{r}
best_par <- par[which.min(result$test_mlogloss_mean), ]

xgb_train <- xgb.DMatrix(data = X_train, label = y_train)


best_xgb_params <- list(booster = "gbtree", 
  eta = best_par[1], gamma = best_par[2], max_depth = best_par[3],  
  subsample = best_par[4], colsample_bytree = 1,
  objective = "multi:softmax", eval_metric = "mlogloss", num_class = 6
)

best_xgb_model <- xgb.train(params = best_xgb_params, data = xgb_train,
                            nrounds = 5000, verbose = 1)

best_y_hat <- predict(best_xgb_model, X_test)
table(best_y_hat)

.best_result <- tibble(`Customer ID` = .data_all$id[.data_all$is_test],
                       `Churn Category` = best_y_hat)

best_result <- result_sample %>% 
  select(`Customer ID`) %>% 
  left_join(.best_result)

write_csv(best_result, "prediction/xgb_eta-0.046_gamma-4.642_md-4_sub-0.75.csv")

y2 <- read_csv("prediction/xgb_eta-0.046_gamma-4.642_md-4_sub-0.75.csv")
table(y2$`Churn Category`)
View(cbind(best_result, y2))
```



```{r}
res1 <- read_csv("prediction/xgb_cv_result.csv")
res2 <- read_csv("prediction/xgb_cv_result2.csv")
res <- rbind(res1, res2)
View(res)
```

```{r}
eta <- seq(0, 0.1, by = 0.02) %>% head(-1)
gamma <- 2:5
max_depth <- c(4, 6)
subsample <- c(0.75)
par <- expand.grid(eta, gamma, max_depth, subsample)

result <- pmap_dfr(par, ~ xgb_cv(X_train, y_train, k = 5,
                                        .eta = ..1, .gamma = ..2, 
                                        .max_depth = ..3, .subsample = ..4))
write_csv(cbind(par, result), "prediction/xgb_cv_result3.csv")
best_par <- par[which.min(result$test_mlogloss_mean), ]
best_par
xgb_train <- xgb.DMatrix(data = X_train, label = y_train)


best_xgb_params <- list(booster = "gbtree", 
  eta = best_par[1], gamma = best_par[2], max_depth = best_par[3],  
  subsample = best_par[4], colsample_bytree = 1,
  objective = "multi:softmax", eval_metric = "mlogloss", num_class = 6
)

best_xgb_model <- xgb.train(params = best_xgb_params, data = xgb_train,
                            nrounds = 5000, verbose = 1)

best_y_hat <- predict(best_xgb_model, X_test)
table(best_y_hat)

.best_result <- tibble(`Customer ID` = .data_all$id[.data_all$is_test],
                       `Churn Category` = best_y_hat)

best_result <- result_sample %>% 
  select(`Customer ID`) %>% 
  left_join(.best_result)
write_csv(best_result, "prediction/xgb_eta-0.04_gamma-5_md-6_sub-0.75.csv")
```


result 3 的資料不要用